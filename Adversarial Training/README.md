## Table of Content (ToC)

- [Adversarial Training](#Adversarial Training)
  - [1 Definition](#71-Definition)
  - [2 Overfitting (a.k.a. Generalization)](#72-Overfitting (a.k.a. Generalization))
    - [2.1 Catastrophic Overfitting](#721-Catastrophic Overfitting)
      - [2.1.1 Underlying Reasons](#7211-Underlying Reasons)
      - [2.1.2 Solutions](#7212-Solutions)  
    - [2.2 Robust Overfitting](#722-Robust Overfitting)
      - [2.2.1 Underlying Reasons](#7221-Underlying Reasons)
      - [2.2.2 Solutions](#7222-Solutions)  
  - [3 Adversarial Robustness Enhancement](#73-Adversarial Robustness Enhancement)
  - [4 Robust Fairness](#74-Robust Fairness)
  - [5 Trade-off between Adversarial Robustness and Standard Accuracy](#75-Trade-off between Adversarial Robustness and Standard Accuracy)
  - [6 Comparison and Connection between Adversarial Training and Randomized Smoothing](#76-Comparison and Connection between Adversarial Training and Randomized Smoothing)
  - [7 Defense against Patch Attack](#77-Defense against Patch Attack)
  - [8 Multi-Attack Robustness](#78-Multi-Attack Robustness)
  - [9 Cross-network/task Adversarial Training](#79-Cross-network/task Adversarial Training)
  - [10 Robust Pre-training and Fine-tuning](#710-Robust Pre-training and Fine-tuning)
  - [11 Adaptive Perturbations](#711-Adaptive Perturbations)
  - [12 Efficiency](#712-Efficiency)
  - [13 Adversarial Training for ViTs and Comparison with CNNs](#713-Adversarial Training for ViTs and Comparison with CNNs)
  - [14 Adversarial Training against Poisoning Attack](#714-Adversarial Training against Poisoning Attack)
    - [14.1 Adversarial Training against Backdoor Attack](#7141-Adversarial Training against Backdoor Attack)
    - [14.2 Adversarial Training against Availability Attack](#7142-Adversarial Training against Availability Attack)
  <!-- - [Citation](#citation) -->

# Adversarial Training

## 1 Definition

- [2023/11] **Adapting Fake News Detection to the Era of Large Language Models.** *Jinyan Su et al. arXiv.* [[paper](https://arxiv.org/abs/2311.04917)]

## 2 Overfitting

### 2.1 Catastrophic Overfitting

##### 2.1.1 Underlying Reasons

- [2023/09] **A Survey of Hallucination in Large Foundation Models.** *Vipula Rawte et al. arxiv.* [[paper](https://arxiv.org/abs/2309.05922)]

##### 2.1.2 Solutions
- [2023/09] **A Survey of Hallucination in Large Foundation Models.** *Vipula Rawte et al. arxiv.* [[paper](https://arxiv.org/abs/2309.05922)]

### 2.2 Robust Overfitting
##### 2.2.1 Underlying Reasons

- [2023/09] **A Survey of Hallucination in Large Foundation Models.** *Vipula Rawte et al. arxiv.* [[paper](https://arxiv.org/abs/2309.05922)]

##### 2.2.2 Solutions
- [2023/09] **A Survey of Hallucination in Large Foundation Models.** *Vipula Rawte et al. arxiv.* [[paper](https://arxiv.org/abs/2309.05922)]

## 3 Adversarial Robustness Enhancement

## 4 Robust Fairness

## 5 Trade-off between Adversarial Robustness and Standard Accuracy

## 6 Comparison and Connection between Adversarial Training and Randomized Smoothing

## 7 Defense against Patch Attack

## 8 Multi-Attack Robustness

## 9 Cross-network/task Adversarial Training

## 10 Robust Pre-training and Fine-tuning

## 11 Adaptive Perturbations

## 12 Efficiency

## 13 Adversarial Training for ViTs and Comparison with CNNs

## 14 Adversarial Training against Poisoning Attack

### 14.1 Adversarial Training against Backdoor Attack

### 14.2 Adversarial Training against Availability Attack
