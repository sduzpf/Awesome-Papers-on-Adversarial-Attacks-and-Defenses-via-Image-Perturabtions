# Adversarial Training
>Adversarial training involves a two-player game between the adversary and the defender to train models with adversarial examples to improve model resilence.

- [1 Definition](#1-Definition)
- [2 Overfitting (Generalization)](#2-Overfitting-Generalization)
  - [2.1 Catastrophic Overfitting](#21-Catastrophic-Overfitting)
    - [2.1.1 Underlying Reasons](#211-Underlying-Reasons)
    - [2.1.2 Solutions](#212-Solutions)  
  - [2.2 Robust Overfitting](#722-Robust-Overfitting)
    - [2.2.1 Underlying Reasons](#221-Underlying-Reasons)
    - [2.2.2 Solutions](#222-Solutions)  
- [3 Adversarial Robustness Enhancement](#3-Adversarial-Robustness-Enhancement)
- [4 Robust Fairness](#4-Robust-Fairness)
- [5 Trade-off between Adversarial Robustness and Standard Accuracy](#5-Trade-off-between-Adversarial-Robustness-and-Standard-Accuracy)
- [6 Comparison and Connection between Adversarial Training and Randomized Smoothing](#6-Comparison-and-Connection-between-Adversarial-Training-and-Randomized-Smoothing)
- [7 Defense against Patch Attack](#7-Defense-against-Patch-Attack)
- [8 Multi-Attack Robustness](#8-Multi-Attack-Robustness)
- [9 Cross-network/task Adversarial Training](#9-Cross-network/task-Adversarial-Training)
- [10 Robust Pre-training and Fine-tuning](#10-Robust-Pre-training-and-Fine-tuning)
- [11 Adaptive Perturbations](#11-Adaptive-Perturbations)
- [12 Efficiency](#12-Efficiency)
- [13 Adversarial Training for ViTs and Comparison with CNNs](#13-Adversarial-Training-for-ViTs-and-Comparison-with-CNNs)
- [14 Adversarial Training against Poisoning Attack](#14-Adversarial-Training-against-Poisoning-Attack)
  - [14.1 Adversarial Training against Backdoor Attack](#141-Adversarial-Training-against-Backdoor-Attack)
  - [14.2 Adversarial-Training-against-Availability-Attack](#142-Adversarial-Training-against-Availability-Attack)
  <!-- - [Citation](#citation) -->

# Adversarial Training

## 1 Definition

- [2023/11] **Adapting Fake News Detection to the Era of Large Language Models.** *Jinyan Su et al. arXiv.* [[paper](https://arxiv.org/abs/2311.04917)]

## 2 Overfitting

### 2.1 Catastrophic Overfitting

##### 2.1.1 Underlying Reasons

- [2023/09] **A Survey of Hallucination in Large Foundation Models.** *Vipula Rawte et al. arxiv.* [[paper](https://arxiv.org/abs/2309.05922)]

##### 2.1.2 Solutions
- [2023/09] **A Survey of Hallucination in Large Foundation Models.** *Vipula Rawte et al. arxiv.* [[paper](https://arxiv.org/abs/2309.05922)]

### 2.2 Robust Overfitting
##### 2.2.1 Underlying Reasons

- [2023/09] **A Survey of Hallucination in Large Foundation Models.** *Vipula Rawte et al. arxiv.* [[paper](https://arxiv.org/abs/2309.05922)]

##### 2.2.2 Solutions
- [2023/09] **A Survey of Hallucination in Large Foundation Models.** *Vipula Rawte et al. arxiv.* [[paper](https://arxiv.org/abs/2309.05922)]

## 3 Adversarial Robustness Enhancement

## 4 Robust Fairness

## 5 Trade-off between Adversarial Robustness and Standard Accuracy

## 6 Comparison and Connection between Adversarial Training and Randomized Smoothing

## 7 Defense against Patch Attack

## 8 Multi-Attack Robustness

## 9 Cross-network/task Adversarial Training

## 10 Robust Pre-training and Fine-tuning

## 11 Adaptive Perturbations

## 12 Efficiency

## 13 Adversarial Training for ViTs and Comparison with CNNs

## 14 Adversarial Training against Poisoning Attack

### 14.1 Adversarial Training against Backdoor Attack

### 14.2 Adversarial Training against Availability Attack
