- [Poisoning Attack](#poisoning-attack)
  - [1. Adversarial Perturbation-based Poisoning Attack](#3-Adversarial Perturbation-based Poisoning Attack)
    - [1.1 Targeted Poisoning Attack](#31-Targeted Poisoning Attack)
    - [1.2 Backdoor/Trojan Attack](#32-Backdoor/Trojan Attack)
      - [1.2.1 Alleviating Hallucination of LLMs](#321-alleviating-hallucination-of-llms)
      - [1.2.2 Improving Safety of LLMs](#322-improving-safety-of-llms)
      - [1.2.3 Detecting LLM-Generated Misinformation](#323-detecting-llm-generated-misinformation)
    - [1.3 Untargeted (a.k.a. Availability, Delusive, Indiscriminate) Attack](#33-Untargeted (a.k.a. Availability, Delusive, Indiscriminate) Attack)
    - [1.4 Transferability](#34-Transferability)
      - [1.4.1 Downstream-agnostic Attack](#341-Downstream-agnostic Attack)
        - [1.4.1.1 Targeted poisoning](#3411-Targeted poisoning)
        - [1.4.1.2 Backdoor attacks](#3412-Backdoor attacks)
        - [1.4.1.3 Untargeted attacks](#3413-Untargeted attacks)
      - [1.5 Imperceptibility](#35-Imperceptibility)
      - [1.6 Label-agnostic Attack](#36-Label-agnostic Attack)
      - [1.7 Poisoning against Defense](#37-Poisoning against Defense)
      - [1.8 Connection between Evasion Attack and Poisoning Attack](#38-Connection between Evasion Attack and Poisoning Attack)
      - [1.9 Poisoning against Vision Transformer](#39-Poisoning against Vision Transformer)
      - [1.10 Efficiency](#310-Efficiency)
  <!-- - [Citation](#citation) -->

# PERTURBATION-BASED Perturbation ATTACK

## 1. Adversarial Perturbation-based Poisoning Attack

### 1.1 Targeted Poisoning Attack

- [2023/11] **Adapting Fake News Detection to the Era of Large Language Models.** *Jinyan Su et al. arXiv.* [[paper](https://arxiv.org/abs/2311.04917)]

### 1.2 Backdoor/Trojan Attack

#### 1.2.1 Alleviating Hallucination of LLMs

##### survey

- [2023/09] **Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models.** *Yue Zhang et al. arxiv.* [[paper](https://arxiv.org/abs/2309.01219)]
- [2023/09] **A Survey of Hallucination in Large Foundation Models.** *Vipula Rawte et al. arxiv.* [[paper](https://arxiv.org/abs/2309.05922)]

### 1.3 Untargeted (a.k.a. Availability, Delusive, Indiscriminate) Attack

### 1.4 Transferability
#### 1.4.1 Downstream-agnostic Attack

##### Targeted poisoning
- [2023/09] **Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models.** *Yue Zhang et al. arxiv.* [[paper](https://arxiv.org/abs/2309.01219)]

##### Backdoor attacks
- [2023/09] **Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models.** *Yue Zhang et al. arxiv.* [[paper](https://arxiv.org/abs/2309.01219)]

##### Untargeted attacks
- [2023/09] **Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models.** *Yue Zhang et al. arxiv.* [[paper](https://arxiv.org/abs/2309.01219)]

### 1.5 Imperceptibility

### 1.6 Label-agnostic Attack

### 1.7 Poisoning against Defense

### 1.8 Connection between Evasion Attack and Poisoning Attack

### 3.9 Poisoning against Vision Transformer

### 3.10 Efficiency


  
